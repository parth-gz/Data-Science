{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33587b49-46f4-4459-baca-66313e16eeab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "#Python libray that parses 'Ubnderstand' large volume of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8f28866-84e2-491b-b3c0-d441659a4836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla\n",
      "is\n",
      "looking\n",
      "at\n",
      "buying\n",
      "U.S\n",
      ",\n",
      "startup\n",
      "for\n",
      "$\n",
      "6\n",
      "million\n"
     ]
    }
   ],
   "source": [
    "nlp=spacy.load('en_core_web_sm')\n",
    "\n",
    "#create a doc object\n",
    "doc=nlp(u'Tesla is looking at buying U.S ,startup for $6 million')\n",
    "\n",
    "#Print each token separately\n",
    "for token in doc:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fbeba37f-cb90-47a7-8b12-306c8527182d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla PROPN\n",
      "is AUX\n",
      "looking VERB\n",
      "at ADP\n",
      "buying VERB\n",
      "U.S PROPN\n",
      ", PUNCT\n",
      "startup VERB\n",
      "for ADP\n",
      "$ SYM\n",
      "6 NUM\n",
      "million NUM\n"
     ]
    }
   ],
   "source": [
    "nlp=spacy.load('en_core_web_sm')\n",
    "\n",
    "#create a doc object\n",
    "doc=nlp(u'Tesla is looking at buying U.S ,startup for $6 million')\n",
    "\n",
    "#Print each token separately\n",
    "for token in doc:\n",
    "    print(token.text,token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ed9e00e4-5a8f-4072-aa63-16e8d361a24d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla PROPN nsubj\n",
      "is AUX aux\n",
      "looking VERB ROOT\n",
      "at ADP prep\n",
      "buying VERB pcomp\n",
      "U.S PROPN dobj\n",
      ", PUNCT punct\n",
      "startup VERB dep\n",
      "for ADP prep\n",
      "$ SYM quantmod\n",
      "6 NUM compound\n",
      "million NUM pobj\n"
     ]
    }
   ],
   "source": [
    "nlp=spacy.load('en_core_web_sm')\n",
    "\n",
    "#create a doc object\n",
    "doc=nlp(u'Tesla is looking at buying U.S ,startup for $6 million')\n",
    "\n",
    "#Print each token separately\n",
    "for token in doc:\n",
    "    print(token.text,token.pos_,token.dep_)\n",
    "    #predict syntatic dependencies\n",
    "    #predict syntactic dependencies involoves identifying the grammatical structure of the sentence\n",
    "    #by determiining how diffrent words relate to each other\n",
    "    #dep_ : It stands for \"dependency\" and provides the type of syntactic relationship (e.g., subject, object, modifier) a token has with another token in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "032e7dcf-0b48-44d4-90d8-f900a83fa4d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x214f5762db0>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x214f5762510>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x214f64a7450>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x214f7319d90>),\n",
       " ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x214f72f5610>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x214f64a7680>)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6e4706c4-cfaa-439c-898a-671c4ccdfc66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b3191b31-5db4-4d30-b1b7-1fae62cf4170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla PROPN nsubj\n",
      "is AUX aux\n",
      "n't PART neg\n",
      "      SPACE dep\n",
      "looking VERB ROOT\n",
      "into ADP prep\n",
      "startups NOUN pobj\n",
      "anymore ADV advmod\n"
     ]
    }
   ],
   "source": [
    "doc2=nlp(u\"Tesla isn't      looking into startups anymore\")\n",
    "\n",
    "for token in doc2:\n",
    "    print(token.text,token.pos_,token.dep_)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f28141a6-3b68-4802-ad19-3c29bd6c6fac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Tesla isn't      looking into startups anymore"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "521f5aad-c555-407c-95ca-a0be8c18dc28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "n't"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "56145f87-7002-4dbe-a8ae-e7b74bb13305",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PART'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pos of individual token\n",
    "doc2[2].pos_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "effe70ac-84d7-4d92-a18f-76a9315e803a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'neg'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dep_ of individual token\n",
    "doc2[2].dep_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaba03ea-5db0-490f-9a41-e2b888ca1c41",
   "metadata": {},
   "source": [
    "28/08/2024\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "19a4b2d0-6e69-4c43-a95e-3e145a12f172",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'negation modifier'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('neg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "42ab8816-711f-44fa-b918-b610224bdcfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'particle'"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain('PART')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8e4a762e-d0a7-4ea2-b953-d6e2e15e1a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "looking\n",
      "look\n"
     ]
    }
   ],
   "source": [
    "#Lemas (the base form at the word\n",
    "print(doc2[4].text)\n",
    "print(doc2[4].lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f91dfd6-e13d-4045-8999-cc4ca8248e40",
   "metadata": {},
   "source": [
    "Spans\n",
    "\n",
    "LArge doc object can be hard to work with . A span is a alice of=d DOC object in the form \n",
    "DOC[start:stop]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "119b8611-567e-4e5d-940f-cb39bcf18898",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc3=nlp(u'Although commonly attributeed to John Lennon from his song \"Beautiful Boy\",\\\n",
    "         the phrase \"Life is what happen to us while we are makin other plans\" was written by \\\n",
    "         cartoonist Allen Saunders and published in REader\\'s Diggest 1957, When Lennon was 17.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6b69abc8-aae5-4ef7-b926-8b95b853697f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Life is what happen to us while we are makin other plans\"\n"
     ]
    }
   ],
   "source": [
    "life_quote=doc3[17:31]\n",
    "print(life_quote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d4d34df0-08c1-4add-aa03-80c374d4782e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.span.Span"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(life_quote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6e90875c-8dc1-460e-bc93-20e15df5071b",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc4=nlp(u'This is first sentence. This is another sentence. This is last sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "feec7d52-5008-413c-acbc-2630d32204fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is first sentence.\n",
      "This is another sentence.\n",
      "This is last sentence\n"
     ]
    }
   ],
   "source": [
    "for sent in doc4.sents:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ed5d8aa3-9049-45ba-bde8-1bea1385681d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We're moving to L.A.!\n"
     ]
    }
   ],
   "source": [
    "#create a string that inclludes opening and closing quatation marks\n",
    "mystring=(\"We\\'re moving to L.\\!\")\n",
    "print(mystring)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c7da2673-7678-4347-b678-dfdda18174aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We | 're | moving | to | L.A. | ! | "
     ]
    }
   ],
   "source": [
    "#create a DOC object and explore the token\n",
    "doc=nlp(mystring)\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text,end=' | ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fbe7da6c-b2f3-49cc-b048-00bd6cbc1954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We | 're | here | to | help | send | snail | - | mail | , | email | support@oursite.com | or | visit | us | at | https://www.oursite.com | ! | "
     ]
    }
   ],
   "source": [
    "doc2=nlp(u\"We're here to help send snail-mail ,email support@oursite.com or visit us at https://www.oursite.com!\")\n",
    "\n",
    "for token in doc2:\n",
    "    print(token.text,end=' | ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "dd7be110-763e-405e-b819-66945cfc6a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "5\n",
      "km\n",
      "NYC\n",
      "cab\n",
      "ride\n",
      "costs\n",
      "$\n",
      "10.30\n"
     ]
    }
   ],
   "source": [
    "doc3=nlp(u'A 5km NYC cab ride costs $10.30')\n",
    "\n",
    "for token in doc3:\n",
    "    print(token.text)  #here the dollar amount is preserved\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "08ff4cad-02e9-4b97-ab37-06f50378a8ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let\n",
      "'s\n",
      "visit\n",
      "St.\n",
      "Louis\n",
      "in\n",
      "the\n",
      "U.S.\n",
      "next\n",
      "year\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "doc4=nlp(u\"Let's visit St.Louis in the U.S. next year.\")\n",
    "\n",
    "for token in doc4:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213dda4f-8df3-44f7-b710-21e07e3156f2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9d6b263-4fae-4dba-9b44-8740511d5875",
   "metadata": {},
   "source": [
    "29/08/2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "be8f1acf-4703-4921-9205-7da8f706f032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple | to | built | a | Hong | Kong | factory | for | $ | 6million | \n",
      "---------------------------------------------------------------\n",
      "Apple - ORG - Companies, agencies, institutions, etc.\n",
      "Hong Kong - GPE - Countries, cities, states\n",
      "6million - MONEY - Monetary values, including unit\n",
      "\n",
      "---------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import spacy \n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "doc8=nlp(u'Apple to built a Hong Kong factory for $6million')\n",
    "\n",
    "for token in doc8:\n",
    "    print(token.text,end=' | ')\n",
    "\n",
    "print('\\n---------------------------------------------------------------')\n",
    "\n",
    "for ent in doc8.ents:\n",
    "    print(ent.text+' - '+ent.label_+' - '+str(spacy.explain(ent.label_)))\n",
    "print('\\n---------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bd1321-e5c0-46ab-ac73-562e49d1ac46",
   "metadata": {},
   "source": [
    "Noun chunks \n",
    "noun chunkls are similar to Doc.ents noun_chunks are another object pro[perty . noun chunks are 'Base noun phrases' first phrases that \n",
    "have a noun as their head "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e1438d59-0f06-4cfb-957b-04c108af40ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autonomous cars\n",
      "insurance liablity\n",
      "manufacturers\n"
     ]
    }
   ],
   "source": [
    "doc9=nlp(u'Autonomous cars shift insurance liablity towards manufacturers')\n",
    "for chunk in doc9.noun_chunks:\n",
    "    print(chunk.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f6104ca6-4c33-4242-bf92-1cd9258d8660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Red cars\n",
      "higher insurance rates\n"
     ]
    }
   ],
   "source": [
    "doc10=nlp(u'Red cars do not carry higher insurance rates')\n",
    "for chunk in doc10.noun_chunks:\n",
    "    print(chunk.text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0da48a-9734-4b67-9162-331137295654",
   "metadata": {},
   "source": [
    "Stemming is a somewhat crude method for cataloging related words. It essentially chops off letters from the end until the stem is reached. While it works fairly well in most cases, English has many exceptions where a more sophisticated process is required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d955488f-8a81-4566-b5d9-e641b0c45963",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the tollkit and fill the porterstemmer library\n",
    "import nltk\n",
    "\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "210129e2-50a2-4366-af25-39b784f52cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_stemmer=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db9ba8d8-4d4e-456b-a8a7-8cef58559eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=['run','runner','running','ran','runs','easily','fairly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6263030b-5472-41fb-a059-8cd3901ef9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run --> run\n",
      "runner --> runner\n",
      "running --> run\n",
      "ran --> ran\n",
      "runs --> run\n",
      "easily --> easili\n",
      "fairly --> fairli\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+' --> ' +p_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "618140f8-3cee-4424-a7c2-cd591bd4197e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the tollkit and fill the snowballstemmer library\n",
    "import nltk\n",
    "\n",
    "from nltk.stem.snowball import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d302b3e-60b8-4e69-9ccb-703d4419e510",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "s_stemmer=SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "00ba5b1d-ece7-4374-b7bb-36d9a10cb501",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=['run','runner','running','ran','runs','easily','fairly']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "93ae2aba-27cc-4497-a0a1-1bab5447fab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run --> run\n",
      "runner --> runner\n",
      "running --> run\n",
      "ran --> ran\n",
      "runs --> run\n",
      "easily --> easili\n",
      "fairly --> fair\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+' --> ' +s_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "020bf3a1-e6be-4280-90c6-8404a35da9cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I --> i\n",
      "am --> am\n",
      "meeting --> meet\n",
      "him --> him\n",
      "tomorrow --> tomorrow\n",
      "at --> at\n",
      "the --> the\n",
      "meeting --> meet\n"
     ]
    }
   ],
   "source": [
    "phrase='I am meeting him tomorrow at the meeting'\n",
    "for word in phrase.split():\n",
    "    print(word+' --> ' +s_stemmer.stem(word))\n",
    "\n",
    "#here meetingf appears twice one in the form of verb and another in the form of nounand yet stemmer treats them equally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d3ad51-1c8f-4673-b752-27bb00cc2b39",
   "metadata": {},
   "source": [
    "In contrast to stemming, lemmatization is a lot more powerful. It looks beyond word reduction and considers a language’s\n",
    "full vocabulary to apply a morphological analysis to words, aiming to remove inflectional endings only and to return the base\n",
    "or dictionary form of a word, which is known as the lemma.\n",
    "\n",
    "\n",
    "\n",
    "Stemming: Quick and often less accurate; may result in non-dictionary words. Example: “running” → “run.”\n",
    "Lemmatization: More accurate and context-aware; produces dictionary words. Example: “running” → “run.”\n",
    "In practical applications, lemmatization might be preferred when the precise meaning and grammatical correctness are important, \n",
    "while stemming might be used for simpler tasks where speed is a priority."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "28896a00-c972-4b57-87ba-767d2b567b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1=nlp(u'I am a runner running in the race beacause I love to run since I ran today')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "44b2d2b3-6122-41b0-98af-e62fa23052ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I \t PRON \t 4690420944186131903 \t I\n",
      "am \t AUX \t 10382539506755952630 \t be\n",
      "a \t DET \t 11901859001352538922 \t a\n",
      "runner \t NOUN \t 12640964157389618806 \t runner\n",
      "running \t VERB \t 12767647472892411841 \t run\n",
      "in \t ADP \t 3002984154512732771 \t in\n",
      "the \t DET \t 7425985699627899538 \t the\n",
      "race \t NOUN \t 8048469955494714898 \t race\n",
      "beacause \t NOUN \t 7794235611920507838 \t beacause\n",
      "I \t PRON \t 4690420944186131903 \t I\n",
      "love \t VERB \t 3702023516439754181 \t love\n",
      "to \t PART \t 3791531372978436496 \t to\n",
      "run \t VERB \t 12767647472892411841 \t run\n",
      "since \t SCONJ \t 10066841407251338481 \t since\n",
      "I \t PRON \t 4690420944186131903 \t I\n",
      "ran \t VERB \t 12767647472892411841 \t run\n",
      "today \t NOUN \t 11042482332948150395 \t today\n"
     ]
    }
   ],
   "source": [
    "for token in doc1:\n",
    "    print(token.text, '\\t',token.pos_, '\\t',token.lemma, '\\t',token.lemma_)\n",
    " \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "ed53c8f0-cd71-4a7a-bc6a-31d1237bb285",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_lemmas(text):\n",
    "    for token in text:\n",
    "        print(f'{token.text:{12}}{token.pos_:{6}} {token.lemma:<{22}} {token.lemma_:{10}}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "36d60b33-c461-46a2-a02b-9e46173866d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There       PRON   2112642640949226496    there     \n",
      "was         VERB   10382539506755952630   be        \n",
      "a           DET    11901859001352538922   a         \n",
      "runner      NOUN   12640964157389618806   runner    \n",
      "ran         VERB   12767647472892411841   run       \n",
      "for         ADP    16037325823156266367   for       \n",
      "life        NOUN   16477424308483498569   life      \n"
     ]
    }
   ],
   "source": [
    "doc1=nlp(u'There was a runner ran for life')\n",
    "show_lemmas(doc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "8085233e-6513-498d-b621-24b27b5748b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I           PRON   4690420944186131903    I         \n",
      "am          AUX    10382539506755952630   be        \n",
      "meeting     VERB   6880656908171229526    meet      \n",
      "him         PRON   1655312771067108281    he        \n",
      "tomorrow    NOUN   3573583789758258062    tomorrow  \n",
      "at          ADP    11667289587015813222   at        \n",
      "the         DET    7425985699627899538    the       \n",
      "meeting     NOUN   14798207169164081740   meeting   \n"
     ]
    }
   ],
   "source": [
    "phrase=nlp(u'I am meeting him tomorrow at the meeting')\n",
    "\n",
    "show_lemmas(phrase)\n",
    "\n",
    "#here meetingf appears twice one in the form of verb and another in the form of noun and yet stemmer treats them diffrently"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b950e61-2f67-459d-84b0-8b7b356d8b6e",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------\n",
    "\n",
    "Stopwords\n",
    "\n",
    "A stop word is a commonly used word (such as “the”, “a”, “an”, or “in”) that a search engine has been programmed to ignore,\n",
    "both when indexing entries for searching and when retrieving them as the result of a search query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "f9476902-b83b-495c-aae0-d6b2c8ada35e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'over', 'behind', 'such', 'many', 'show', 'down', '‘ll', 'beyond', 'therefore', 'any', 'her', 'due', 'name', 'she', 'nevertheless', 'move', 'not', 'every', 'therein', 'without', 'doing', 'someone', 'own', 'more', 'may', 'thence', 'rather', 'either', 'but', 'then', 'further', 'anything', 'moreover', 'take', 'enough', 'you', 'give', 'next', 'however', 'using', 'here', 'whom', 'per', 'call', 'most', 'hundred', 'or', 'back', 'go', 'becomes', 'five', 'namely', 'never', 'less', 'above', 'at', 'yours', 'except', 'amount', 'fifty', 'they', 'this', '‘m', 'done', 'noone', \"'d\", 'all', 'mostly', 'whither', 'ever', 'two', 'well', 'indeed', 'whereafter', '’s', 'will', 'when', 'and', 'why', 'i', 'some', 'hers', 'ca', '’re', 'though', 'keep', 'herein', 'if', 'same', 'whereby', 'eight', 'say', 'whereas', 'fifteen', 'about', 'again', 'as', 'sometime', 'three', 'is', \"'s\", 'those', 'upon', 'n‘t', 'somehow', 'themselves', 'nobody', 'bottom', 'quite', 'meanwhile', 'thereupon', 'in', 'least', 'itself', 'thereafter', 'wherein', 'really', 'while', 'seem', 'nowhere', 'last', 'across', 'third', '‘d', '’ll', 'it', 'that', 'too', 'into', 'them', 'formerly', 'must', 'out', 'together', 'eleven', 'both', 'cannot', 'amongst', 'side', 'up', 'with', 'were', 'he', 'via', 'there', 'something', 'being', 'only', 'elsewhere', 'been', 'against', 'always', 'nor', 'by', 'whoever', 'besides', 'still', 'me', 'might', 'wherever', 'serious', 'seems', 'used', 'ten', '’d', 'four', 'around', 'former', 'already', 'thru', '’ve', 'my', 'whatever', \"n't\", 'latterly', 'often', 'few', 'empty', 'everywhere', 'once', 'yet', 'yourselves', 'ours', 'through', 'regarding', 'their', 'very', 'whence', 'hence', '‘re', 'becoming', 'also', 'can', 'towards', 'have', 'beforehand', 'an', 'anyone', 'throughout', 'among', 'six', 'what', 'himself', 'get', 'on', 'just', 'part', 'than', 'where', 'somewhere', 'ourselves', 'latter', 'for', 'these', 'thus', 'became', 'whereupon', 'below', 'several', 'neither', 'of', 'hereupon', 'please', 'full', '’m', 'whole', 'until', 'to', 'although', 'onto', 'should', 'sixty', 'had', 'front', 'twenty', 'whether', 'unless', 'did', 'each', 'does', 'off', 'would', 'so', 'first', 'are', 'who', '‘ve', 'myself', 'since', 're', 'herself', 'alone', 'which', 'afterwards', 'no', 'am', 'another', 'between', 'beside', 'our', 'within', 'whenever', 'hereby', 'yourself', 'everything', 'how', \"'ll\", 'see', 'the', 'put', 'made', 'could', 'us', 'your', 'sometimes', 'a', 'twelve', 'else', 'other', 'mine', 'whose', 'from', \"'m\", 'anywhere', 'everyone', 'has', 'him', 'forty', 'almost', 'much', 'its', 'anyway', 'nine', 'seemed', 'n’t', 'hereafter', 'after', 'various', \"'re\", 'under', 'others', '‘s', 'because', \"'ve\", 'otherwise', 'we', 'none', 'before', 'perhaps', 'now', 'along', 'during', 'even', 'anyhow', 'thereby', 'top', 'become', 'do', 'be', 'nothing', 'seeming', 'make', 'his', 'toward', 'one', 'was'}\n"
     ]
    }
   ],
   "source": [
    "print(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "150c220f-9a33-4b97-aec2-88f7bcf18e3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "c51f0d9b-ff46-4a7e-ad78-1da7260ffec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to see word is stop word or not\n",
    "nlp.vocab['myself'].is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "168f843d-b3ab-4bd0-93ff-de244d78ce8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to see word is stop word or not\n",
    "nlp.vocab['mystery'].is_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "90c2d6bc-92f5-45dd-9d6c-cb7eeaa1d987",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add the word to the set of stop words ,Use lowercase\n",
    "\n",
    "nlp.Defaults.stop_words.add('btw')\n",
    "\n",
    "#set the stop_wordtag on the lexeme\n",
    "nlp.vocab['btw'].is_stop=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "5e05c84e-1044-4ec3-982b-50250c3d9cf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "327"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "00261874-5e8f-483e-a577-35fc2b32437c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove the word to the set of stop words ,Use lowercase\n",
    "\n",
    "nlp.Defaults.stop_words.remove('beyond')\n",
    "\n",
    "#set the stop_wordtag on the lexeme\n",
    "nlp.vocab['btw'].is_stop=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "07558ee2-13ca-4611-b439-2bcb0e1553af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nlp.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bd1492-7ab0-4fc0-b838-2a3476709494",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
